"""CK-Download NFOç”Ÿæˆå™¨ã€‚

ä¸“é—¨ç”¨äºCK-Downloadç½‘ç«™çš„NFOæ–‡ä»¶ç”Ÿæˆã€‚
"""

import re
from typing import Optional
from bs4 import BeautifulSoup
from datetime import datetime

from ..core.base_generator import BaseNfoGenerator
from ..core.movie_data import MovieData
from ..core.exceptions import ScrapingError, NetworkError


class CkDownloadNfoGenerator(BaseNfoGenerator):
    """CK-Downloadç½‘ç«™çš„NFOç”Ÿæˆå™¨ã€‚
    
    æ”¯æŒä»CK-Downloadç½‘ç«™çˆ¬å–ç”µå½±ä¿¡æ¯å¹¶ç”Ÿæˆæ ‡å‡†NFOæ–‡ä»¶ã€‚
    """
    
    @property
    def site_name(self) -> str:
        """è¿”å›æ”¯æŒçš„ç½‘ç«™åç§°ã€‚"""
        return "CK-Download"
    
    @property
    def site_domain(self) -> str:
        """è¿”å›æ”¯æŒçš„ç½‘ç«™åŸŸåã€‚"""
        return "ck-download"
    
    def extract_product_id(self, url: str) -> Optional[str]:
        """ä»CK-Download URLä¸­æå–äº§å“IDã€‚
        
        Args:
            url: ç”µå½±URL
            
        Returns:
            æ‰¾åˆ°åˆ™è¿”å›äº§å“IDï¼Œå¦åˆ™è¿”å›None
            
        æ”¯æŒçš„URLæ ¼å¼ï¼š
        - https://ck-download.com/product/detail/12345
        """
        match = re.search(r"product/detail/(\d+)", url)
        if match:
            return match.group(1)
        return None
    
    def scrape_movie_info(self, url: str) -> bool:
        """ä»CK-Download URLçˆ¬å–ç”µå½±ä¿¡æ¯ã€‚
        
        Args:
            url: ç”µå½±URL
            
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
            
        Raises:
            ScrapingError: çˆ¬å–å¤±è´¥æ—¶æŠ›å‡º
            NetworkError: ç½‘ç»œè¯·æ±‚å¤±è´¥æ—¶æŠ›å‡º
        """
        print("ğŸš€ å¼€å§‹å°è¯•è·å–å½±ç‰‡ä¿¡æ¯...")
        
        try:
            response = self.make_request(url)
            response.encoding = "utf-8"
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Extract basic information
            title = self._extract_title(soup)
            product_number = self._extract_product_number(soup)
            
            # Combine product number with title
            if product_number:
                title = f"{product_number} {title}".strip()
            
            # åˆå§‹åŒ–ç”µå½±æ•°æ®
            self.movie_data = MovieData(
                title=title,
                original_title=title,
                product_id=self.extract_product_id(url) or "26966",
                year=self._extract_year(soup),
                plot=self._extract_plot(soup),
                outline="",  # æ¦‚è¦é»˜è®¤ç©º
                genres=self._extract_tags(soup),
                runtime=self._extract_runtime(soup),
                studio=self.site_name,
                release_date=self._extract_premiered(soup),
                poster=self._extract_poster(soup),  # å°é¢å›¾ç‰‡URL
                imdb_id=f"CK-{self.extract_product_id(url) or '26966'}"
            )
            
            # æ·»åŠ æ¼”å‘˜ä¿¡æ¯
            actors = self._extract_actors(soup)
            for actor_info in actors:
                self.movie_data.add_actor(actor_info["name"], "Actor")
            
            # æ·»åŠ æ ‡ç­¾ï¼ˆç¬¬ä¸€ä¸ªä¸ºimdbidï¼‰
            self.movie_data.tags.extend([self.movie_data.imdb_id, "å›½äº§", "CK-Download"])
            
            # ä½¿ç”¨æ ‡å‡†æ¨¡æ¿
            self.nfo_template = "standard"
            
            print("âœ… åŸºæœ¬ä¿¡æ¯è·å–å®Œæˆ")
            return True
            
        except NetworkError:
            raise
        except Exception as e:
            raise ScrapingError(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ä»é¡µé¢æå–ç”µå½±æ ‡é¢˜ã€‚
        
        å¯è®¾ç½®çš„å€¼ï¼šä»»æ„ç”µå½±æ ‡é¢˜æ–‡æœ¬
        """
        title_elem = soup.select_one("div#Contents h3")
        return title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
    
    def _extract_product_number(self, soup: BeautifulSoup) -> str:
        """ä»é¡µé¢æå–äº§å“ç¼–å·ã€‚
        
        å¯è®¾ç½®çš„å€¼ï¼šå¦‚CK-001ã€CK-002ç­‰æ ¼å¼
        """
        num_td = soup.find("th", string="ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒŠãƒ³ãƒãƒ¼")
        if num_td and num_td.find_next("td"):
            return num_td.find_next("td").get_text(strip=True)
        return ""
    
    def _extract_tags(self, soup: BeautifulSoup) -> list:
        """ä»é¡µé¢æå–æ ‡ç­¾/ç±»å‹ã€‚
        
        å¯è®¾ç½®çš„å€¼ï¼š
        - å‰§æƒ…ã€çˆ±æƒ…ã€åŠ¨ä½œã€å–œå‰§ç­‰ç±»å‹æ ‡ç­¾
        - å›½äº§ã€æ—¥æœ¬ã€éŸ©å›½ç­‰åœ°åŒºæ ‡ç­¾
        - å…¶ä»–è‡ªå®šä¹‰æ ‡ç­¾
        """
        tags = []
        category_div = soup.select_one("div.prod_category")
        if category_div:
            for a in category_div.select("a"):
                text = a.get_text(strip=True)
                if text:
                    tags.append(text)
        return tags if tags else ["å‰§æƒ…", "çˆ±æƒ…"]
    
    def _extract_year(self, soup: BeautifulSoup) -> str:
        """ä»é¡µé¢æå–å¹´ä»½ã€‚
        
        ä»å‘å¸ƒæ—¥æœŸè‡ªåŠ¨è§£æï¼Œæ ¼å¼ï¼šYYYY
        """
        premiered_date = self._extract_premiered(soup)
        if premiered_date:
            return premiered_date.split("-")[0]
        return str(datetime.now().year)
    
    def _extract_plot(self, soup: BeautifulSoup) -> str:
        """ä»é¡µé¢æå–å‰§æƒ…ç®€ä»‹ã€‚
        
        å¯è®¾ç½®çš„å†…å®¹ï¼š
        - è¯¦ç»†çš„å‰§æƒ…æè¿°
        - æ”¯æŒå¤šè¡Œæ–‡æœ¬å’Œæ¢è¡Œ
        - æ”¯æŒä¸­æ–‡ã€æ—¥æ–‡ã€è‹±æ–‡ç­‰å¤šè¯­è¨€
        - CDATAæ ¼å¼è¾“å‡º
        """
        intro_div = soup.select_one("div.intro_text")
        
        if intro_div:
            paragraphs = []
            p_elements = intro_div.find_all("p")
            
            for p in p_elements:
                text = p.get_text().strip()
                if text:
                    # å¤„ç†æ¢è¡Œç¬¦
                    br_elements = p.find_all("br")
                    if br_elements:
                        text = p.get_text("\n", strip=True)
                    paragraphs.append(text)
            
            if paragraphs:
                return "\n\n".join(paragraphs)
        
        return "æš‚æ— å‰§æƒ…ç®€ä»‹"
    
    def _extract_premiered(self, soup: BeautifulSoup) -> str:
        """ä»é¡µé¢æå–é¦–æ˜ æ—¥æœŸã€‚
        
        æ ¼å¼ï¼šYYYY-MM-DD
        ç”¨äºreleasedateå’Œpremieredå­—æ®µ
        """
        date_div = soup.select_one("div.add_info div.date")
        if date_div:
            date_text = date_div.get_text().strip()
            date_match = re.search(r"(\d{4})\.(\d{2})\.(\d{2})", date_text)
            if date_match:
                year, month, day = date_match.groups()
                return f"{year}-{month}-{day}"
        
        current_year = datetime.now().year
        return f"{current_year}-01-01"
    
    def _extract_actors(self, soup: BeautifulSoup) -> list:
        """ä»é¡µé¢æå–æ¼”å‘˜ä¿¡æ¯ã€‚
        
        æ¼”å‘˜å§“åå¯è®¾ç½®çš„å€¼ï¼š
        - ä¸­æ–‡å§“å
        - æ—¥æ–‡å§“å
        - è‹±æ–‡å§“å
        - è‰ºåæˆ–æ˜µç§°
        """
        # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦å®ç°
        # åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæ‚¨éœ€è¦æå–çœŸå®çš„æ¼”å‘˜ä¿¡æ¯
        return [
            {"name": "æ¼”å‘˜1"},
            {"name": "æ¼”å‘˜2"}
        ]
    
    def _extract_runtime(self, soup: BeautifulSoup) -> str:
        """ä»é¡µé¢æå–æ—¶é•¿ã€‚
        
        æ ¼å¼ï¼šåˆ†é’Ÿæ•°ï¼ˆå¦‚ï¼š120ï¼‰
        """
        # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦å®ç°
        # åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæ‚¨éœ€è¦æå–çœŸå®çš„æ—¶é•¿ä¿¡æ¯
        return "120"
    
    def _extract_poster(self, soup: BeautifulSoup) -> str:
        """ä»é¡µé¢æå–å°é¢å›¾ç‰‡URLã€‚
        
        ä»…ç”¨äºç”Ÿæˆå°é¢ï¼Œä¸åœ¨NFOä¸­è¾“å‡ºã€‚
        
        å¯è®¾ç½®çš„å€¼ï¼š
        - å®Œæ•´çš„å›¾ç‰‡URL
        - ç›¸å¯¹è·¯å¾„ï¼ˆä¼šè‡ªåŠ¨è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼‰
        """
        # æŸ¥æ‰¾å°é¢å›¾ç‰‡
        img_selectors = [
            ".poster img",
            ".cover img",
            ".thumbnail img",
            "img[class*='cover']",
            "img[class*='poster']"
        ]
        
        for selector in img_selectors:
            img_elem = soup.select_one(selector)
            if img_elem:
                src = img_elem.get('src') or img_elem.get('data-src')
                if src:
                    return src if src.startswith('http') else f"https:{src}"
        
        return ""